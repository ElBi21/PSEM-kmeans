\documentclass[11pt, journal]{IEEEtran}

\usepackage{lipsum}
\usepackage[T1]{fontenc}
\usepackage{fouriernc}
\usepackage{cases}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[noadjust]{cite}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{fontawesome5}
\usepackage{tcolorbox}
\usepackage[dvipsnames]{xcolor}
\usepackage[ruled]{algorithm2e}
%\usepackage{biblatex}

%\addbibresource{bibliography.bib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    anchorcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\definecolor{codeCOMMENT}{HTML}{649541}
\definecolor{codeSTRING}{HTML}{37864A}
\definecolor{codeKEY}{HTML}{C678DD}
\definecolor{codeFUNC}{HTML}{61AFEF}
\definecolor{codeVALUES}{HTML}{C69438}

\newcommand{\eq}{\; = \;}
\newcommand{\nwl}{

\vspace{11pt}

}
\newcommand{\centered}[2]{\begin{tabular}{#1} #2 \end{tabular}}

\lstdefinestyle{standstyle}{
    commentstyle=\color{codeCOMMENT},
    keywordstyle=\bfseries\color{codeKEY},
    numberstyle=\scriptsize\ttfamily\color{codeVALUES},
    stringstyle=\color{codeSTRING},
    basicstyle=\ttfamily\linespread{1}\scriptsize\color{black!80},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    xleftmargin=15pt,
}

\lstset{style=standstyle}

\DeclareMathAlphabet{\mathcal}{OMS}{zplm}{m}{n}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\commentalg[1]{\textcolor{ForestGreen}{{\footnotesize #1}}}
\SetCommentSty{commentalg}

\title{$k$-means Is All You Need}
\author{Leonardo Biason ($2045751$) \quad Alessandro Romania ($2046144$) \quad Davide De Blasio ($2082600$)}

\begin{document}

\maketitle

\begin{abstract}
    The $k$-means algorithm is a well known clustering algorithm, which is often used in unsupervised learning settings. However, the algorithm requires to perform multiple times the same operation on the data, and it can greatly benefit from a parallel implementation, so that to maximize the throughput and reduce computation times. With this project, we propose some possible implementations, based on some libraries that are considered to be the \text{de-facto} standard when it comes to writing multithreaded or parallel code, and we will discuss also the results of such implementations
\end{abstract}

\begin{keywords}
    Sapienza, ACSAI, Multicore Programming
\end{keywords}
\nwl
\begin{tcolorbox}[colback = Purple!20, colframe = Purple!40]
    \begin{center}
        \faIcon{github} Check our repository \href{https://www.github.com/ElBi21/PSEM-kmeans}{on GitHub}
        \verb|ElBi21/PSEM-kmeans|
    \end{center}
\end{tcolorbox}

\section{Introduction}

When talking about clustering and unsupervised learning, it's quite common to hear about the $k$-means algorithm, and for good reasons: it allows to efficiently cluster a dataset of $d$ dimensions, and it employs the notion of convergence in order to do so. This, computationally speaking, means to repeat some operations over and over again until some stopping conditions are met.

\nwl

The algorithm is not perfect though, and presents some issues:
\begin{itemize}
    \item [1)] the algorithm is fast in clustering, but we cannot be certain that it clusters \textit{well};
    \item [2)] the algorithm doesn't work with non-linear clusters;
    \item [3)] the initialization can make a great impact in the final result.
\end{itemize}
\nwl
Many people prefer to use other clustering methods, such as the fitting of Gaussian Mixture Models. Albeit not being perfect, $k$-means still works well in simple, linear clusters. For the sake of this project, we are going to consider a vanilla $k$-means algorithm with Lloyd's initialization (the first $k$ centroids will be selected randomly).

\subsection{Algorithm structure}

The $k$-means algorithm can be described with the following pseudocode, where $X$ is the set of data points, $C = \{\mu_1, \; \mu_2, \; ..., \; \mu_k \}$ is the set of centroids and $Y$ is the set of assignments:

\begin{algorithm}
    \label{alg:kmeans}
    \LinesNumbered
    \tcp{Initialize the centroids}
    \For{$k$ in $[1, \; |C|]$}{
        $\mu_k \gets \text{a random location in the input space}$
    }
    \BlankLine
    \While{$\text{convergence hasn't been reached}$}{
        \tcp{Assign each point to a cluster}
        \For{$i$ in $[1, \; |X|]$}{
            $y_i \gets \argmin_k \left(\norm{\mu_k - x_i}\right)$ 
        }
        \BlankLine
        \tcp{Compute the new position of each centroid}
        \For{$k$ in $[1, \; |C|]$}{
            $\mu_k \gets \textsc{Mean}(\{ \; x_n : z_n = k \; \})$
        }
    }
    \tcp{Return the centroids}
    \Return $Y$

    \caption{$k$-means (Lloyd's initialization)}
\end{algorithm}

The algorithm consists of 4 main blocks:
\begin{itemize}
    \item the \textbf{initialization block}, where all the centroids will receive a starting, random position (as per Lloyd's method);
    \item the \textbf{assignment block}, where the Euclidean distance between a point and all centroids is computed, for all centroids. The point will be assigned to a cluster depending on the following operation:
    \[ \argmin_k \left(\norm{\mu_k - x_i}\right) \]

    \item the \textbf{update block}, where the position of the centroids is updated, and the new position of a centroid $\mu_k$ is equal to the mean of all the data points positions belonging to cluster $k$
\end{itemize}

\subsection{Sequential Code Bottlenecks}

For implementing the $k$-means algorithm, we will base all the codebase upon the project made from professors Diego García-Álvarez and Arturo Gonzalez-Escribano from the University of Valladolid. The code shown in this subsection is taken from their project, although slightly adapted for giving enough context in the code snippets.
\nwl
We described before the overall structure of the $k$-means algorithm: we will now proceed to examine its two main bottlenecks. As we can see from Algorithm \ref{alg:kmeans}, we have two main blocks that may cause performance issues: the \textbf{assignment block} and the \textbf{update block}.
\nwl
The first \textbf{for} block in the \textbf{initialization step} does not represent a major bottleneck, since it just needs to assign a random location to each of the $K$ centroids. It can be parallelized, but it won't help as much as parallelizing the two steps mentioned before.
\nwl
The second \textbf{for} block represents the \textbf{assignment step}, which is, unlike the initialization step, computationally expensive: for each point, the algorithm will have to compute the euclidean distance (here onwards denotes as $\ell_2$) between said point and all centroids $\mu_k \in C$, and select the lowest distance. This will determine the cluster of the point. In a C program, this may be accomplished with the following piece of code:
\nwl
\begin{lstlisting}[language = C]
int cluster;
// For each point...
for(i = 0; i < points_number; i++) {
    class = 1;
    minDist = FLT_MAX;
    // For each cluster...
    for(j = 0; j < K; j++) {
        // Compute the distance
        dist = l2_norm(&data[i*samples], &centroids[j*samples], samples);

        // If the distance is the lowest so far, replace it
        if(dist < minDist) {
            minDist = dist;
            class = j+1;
        }
    }
    
    // If the class is different from before, add a change to the counter
    if(classMap[i] != class) {
        changes++;
    }

    classMap[i]=class;
}\end{lstlisting}

Notice the presence of the two nested \textbf{for} loops: sequentially, they would take a time of $O(|X| \cdot |C|)$, which may be optimized just by taking a simple single instruction multiple data approach (indeed, with $m > 1$ different processes or threads, it would take a time of $O\left(\frac{|X| \cdot |C|}{m}\right)$ each, which is already better than the first option).
\nwl
The third \textbf{for} loop represents the update step, which also is computationally expensive: we would need to perform the mean of the coordinates of all the points belonging to a cluster $\mu_k$. This implies that all the coordinates of the points must be first summed, and then averaged on the number of points being classified to $\mu_k$. An implementation in the C language would look like the following:
\nwl
\begin{lstlisting}[language = C]
// For each point...
for (i = 0; i < lines; i++) {
    point_class = classMap[i];
    // Add 1 to the points classified for class k
    pointsPerClass[point_class - 1] += 1;

    // For each dimension...
    for(j = 0; j < samples; j++) {
        // ...add it to a table for summing and averaging
        auxCentroids[(point_class - 1) * samples + j] += data[i * samples + j];
    }
}

for (i = 0; i < K; i++) {
    for (j = 0; j < samples; j++) {
        // Average all dimensions
        auxCentroids[i * samples + j] /= pointsPerClass[i];
    }
}

maxDist = FLT_MIN;
for (i = 0; i < K; i++) {
    // Compute the moving distance, as a convergence check
    distCentroids[i] = euclideanDistance(&centroids[i * samples], &auxCentroids[i * samples], samples);
    if (distCentroids[i] > maxDist) {
        maxDist = distCentroids[i];
    }
}\end{lstlisting}

\section{Parallelizing with MPI}

\section{Parallelizing with OpenMP}

To enhance the performance of the $k$-means algorithm, we employed OpenMP for parallelization. OpenMP is a widely-used API for multi-platform shared-memory parallel programming. Our primary goal was to leverage multiple CPU cores to expedite the computationally intensive parts of the algorithm: the \textbf{cluster assignment} and \textbf{centroid update} steps. The number of threads can be optionally specified as a parameter; if not provided, a default value (i.e. 8 threads) is used for parallel execution.

\subsection{OpenMP Implementation Approach}

\textbf{1. Cluster Assignment:}  
The cluster assignment step is inherently parallelizable since each data point can be processed independently. We utilized the \texttt{\# pragma omp parallel for} directive to distribute the loop iterations across multiple threads.
\begin{itemize}
\item \textbf{Reduction Clause}: The \texttt{reduction(+ : changes)} clause safely aggregates the total number of cluster reassignments across threads. This avoids race conditions by creating thread-private copies of \texttt{changes} that are combined after the loop.
\item \textbf{Dynamic Scheduling}: The \texttt{schedule(dynamic, 16)} clause improves load balancing by allowing threads to dynamically claim "chunks" of 16 iterations.
\end{itemize}

\textbf{2. Centroid Update:}  
The centroid update step involves two parallel phases: accumulating cluster sums and normalizing centroids. Here, we addressed potential race conditions through privatization and synchronization:
\begin{itemize}
\item \textbf{Privatization with Local Buffers}: Each thread maintains private copies of \texttt{local\_pointsPerClass} (count of points assigned to the cluster) and \texttt{local\_auxCentroids} (cumulative sum of data points for each cluster). This eliminates races during local accumulation, as threads operate on isolated data.
\item \textbf{Critical Section for Global Aggregation}: After local accumulation, a \texttt{\#pragma omp critical} region safely merges thread-local results into global \texttt{pointsPerClass} and \texttt{auxCentroids}. While critical sections incur synchronization overhead, they are used sparingly here—once per thread—minimizing contention.
\item \textbf{Parallel Mean Calculation}: The final centroid normalization loop (\texttt{\#pragma omp parallel for}) parallelizes the division by \texttt{pointsPerClass}, avoiding race conditions since each cluster is processed independently.
\end{itemize}
An alternative approach that we experimented with uses the reduction feature to eliminate the need for explicit synchronization.
\begin{lstlisting}[language = C]
#pragma omp parallel for reduction(+ : pointsPerClass[ : K], auxCentroids[ : K * samples])
		for (i = 0; i < lines; i++)
		{
			int class = classMap[i] - 1;
			pointsPerClass[class]++;
			for (int j = 0; j < samples; j++)
			{
				auxCentroids[class * samples + j] += data[i * samples + j];
			}
		}\end{lstlisting}
The reduction clause automates the process of merging thread-local results into shared variables. By reducing both \texttt{pointsPerClass} and \texttt{auxCentroids}, the need for a critical section is removed, potentially lowering synchronization overhead.
However, we chose the original approach with explicit privatization and critical regions to demonstrate proficiency with OpenMP synchronization and race condition management. 

The maximum centroid displacement (\texttt{maxDist}) is computed serially. While this loop could be parallelized using \texttt{\#pragma omp parallel for reduction(max : maxDist)}, we retained the serial approach for simplicity, as its computational cost is negligible compared to other steps and because the number of clusters $k$ is typically small, the overhead associated with parallelization outweighs the potential performance gains.

\subsection{Solutions Implemented}
\begin{itemize}
\item \textbf{Reduction for Scalars}: The \texttt{changes} variable uses a \texttt{reduction} clause, which is more efficient than atomic operations for scalar summation. OpenMP handles private copies and post-loop merging automatically.
\item \textbf{Privatization and Critical Sections}: For array/matrix updates (e.g., \texttt{auxCentroids}), thread-local buffers reduce the need for fine-grained synchronization. The critical section ensures safe aggregation with minimal overhead, as it is invoked only once per thread.
\item \textbf{Atomic Operations vs. Critical Regions}: While atomic operations (e.g., \texttt{\#pragma omp atomic}) could replace the critical section for scalar increments (\texttt{pointsPerClass}), they would be inefficient for matrix additions (\texttt{auxCentroids}) due to repeated fine-grained locking. Privatization strikes a better balance between correctness and performance.
\end{itemize}

\subsection{Performance Considerations}

\begin{itemize}
\item \textbf{Critical Section Overhead}: The single critical section per thread during centroid aggregation has negligible cost compared to the computational work, as merging local buffers is a minor operation.
\item \textbf{Memory Efficiency}: Privatizing \texttt{local\_auxCentroids} per thread increases memory usage proportionally to the number of threads. However, this is manageable for moderate thread counts and cluster sizes.
\item \textbf{Dynamic Scheduling Trade-off}: While dynamic scheduling improves load balancing, it introduces overhead for chunk management. A chunk size of 16 was empirically chosen to balance parallelism and scheduling latency.
\end{itemize}

\subsection{Conclusion}

The OpenMP-based parallelization of the $k$-means algorithm effectively mitigates race conditions through a combination of reduction clauses, thread-local privatization, and strategic critical sections. These strategies not only ensured the correctness of the computations but also optimized performance by minimizing synchronization overhead.

\section{Parallelizing with CUDA}

\section{Interlacing Multi-processing with Multi-threading}

\subsection{MPI and OpenMP}
\subsection{MPI and CUDA}

\section{Performance Analysis}

\section{Conclusions}

%\cite{7780459}

%\printbibliography

\end{document}